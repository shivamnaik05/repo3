GET http://localhost:8083/connectors/CamelTelegramSourceConnector/status

GET http://localhost:8083/connector-plugins

GET : http://localhost:8082/subjects/subjectname/versions/

POST : http://localhost:8083/connectors/
	Body:
	 {
        "name": "CamelTelegramSourceConnector",
        "config": {
 "connector.class":"org.apache.camel.kafkaconnector.telegram.CamelTelegramSourceConnector",
 "key.converter":"org.apache.kafka.connect.storage.StringConverter",
"value.converter":"org.apache.kafka.connect.storage.StringConverter",
"topics":"mytelegramtopic",
"camel.source.endpoint.authorizationToken":"1586976426:AAEnGEVZNvWyD5QeJ1pVhj7_ttsfsGoBtNo",
"camel.source.path.type":"bots"

    }
      }
	  
########################################
{
    "name": "spool-file-connector-02",
    "config":
    {
        "connector.class": "com.github.jcustenborder.kafka.connect.spooldir.SpoolDirLineDelimitedSourceConnector",
        "topic": "topic02",
        "input.path": "C:\\shiva\\kafka\\file-spool\\input",
        "input.file.pattern": "^.*$",
        "error.path": "C:\\file-spool\\error",
        "finished.path": "C:\\file-spool\\finished",
		"schema.generation.enabled":"true",
		"schema.generation.key.name":"Key",
		"schema.generation.value.name":"Envelop",
		 "value.converter": "org.apache.kafka.connect.storage.StringConverter",
        "key.converter": "org.apache.kafka.connect.storage.StringConverter",
		"value.converter.schema.registry.url":"http://localhost:8082/",
		"key.converter.schema.registry.url":"http://localhost:8082/",
		"file.minimum.age.ms": "10000"
    }
}

#########################################################################

 {
    "name": "card-auths-spooldir-connector",
   "config":{
    "connector.class": "com.github.jcustenborder.kafka.connect.spooldir.SpoolDirLineDelimitedSourceConnector",
    "topic": "card-auths-transactions",
    "input.path": "C:/opt/kafka/file-spool/input",
    "input.file.pattern": "^.*$",
    "error.path": "C:/opt/kafka/file-spool/error",
    "finished.path": "C:/opt/kafka/file-spool/finished",
   "value.converter": "io.confluent.connect.avro.AvroConverter",
   "value.converter.schema.registry.url":"http://localhost:8082/",
    "file.minimum.age.ms": "10000"
    }
    }
	
########################################################
{
        "name": "connector",
        "config": {
  "connector.class": "io.debezium.connector.sqlserver.SqlServerConnector",
  "database.hostname": "hostname",
  "database.port": "1234",
  "database.user": "usera",
  "database.password": "password",
  "database.dbname": "dbname",
  "database.server.name": "servername",
  "table.whitelist": "db.tablename",
  "database.history.kafka.bootstrap.servers":"localhost:9092",
  "database.history.kafka.topic": "dbhistory.topicname",
  "snapshot.mode":"initial_schema_only"
        }
      }
	  
	  
################################################################
Download the following two files and place at C:\opt\kafka\confluent-5.2.0\bin\windows - 
schema-registry-start.bat
schema-registry-run-class.bat
The above two files are command line utilities to start schema registry, the out-of-the-box Confluent package does not provide them; the files are referenced from following source - 
https://github.com/renukaradhya/confluentplatform/tree/master/bin/windows


####################################################
Start-up

Open command prompt and  Navigate to C:\opt\kafka\confluent-5.2.0\bin\windows 

>start kafka using following sequence of commands

Start kafka using following commands - 

Start zookeeper - 
zookeeper-server-start.bat ..\..\etc\kafka\zookeeper.properties
Start kafka server - 
kafka-server-start.bat ..\..\etc\kafka\server.properties
Start schema registry - 
schema-registry-start.bat ..\..\etc\schema-registry\schema-registry.properties
Start connector (distributed mode)- 
connect-distributed.bat ..\..\etc\kafka\connect-distributed.properties



#############################
.\kafka-avro-console-consumer.bat
--property schema.registry.url=http://localhost:8082
--property print.key=true
--property print.schema.ids=true
--property schema.id.separator=: 
--bootstrap-server localhost:9092
--formatter io.confluent.kafka.formatter.AvroMessageFormatter
--topic <topic name>--from-beginning


##############################3
plugin.path=C:\kafka\\confluent-5.2.0\\share\\kafka\\plugins

###############################
connect-distributed.properties

@echo off
rem Licensed to the Apache Software Foundation (ASF) under one or more
rem contributor license agreements.  See the NOTICE file distributed with
rem this work for additional information regarding copyright ownership.
rem The ASF licenses this file to You under the Apache License, Version 2.0
rem (the "License"); you may not use this file except in compliance with
rem the License.  You may obtain a copy of the License at
rem
rem    http://www.apache.org/licenses/LICENSE-2.0
rem
rem Unless required by applicable law or agreed to in writing, software
rem distributed under the License is distributed on an "AS IS" BASIS,
rem WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
rem See the License for the specific language governing permissions and
rem limitations under the License.

IF [%1] EQU [] (
	echo USAGE: %0 connect-distributed.properties
	EXIT /B 1
)

SetLocal
rem Using pushd popd to set BASE_DIR to the absolute path
pushd %~dp0..\..
set BASE_DIR=%CD%
popd

sx
IF ["%KAFKA_LOG4J_OPTS%"] EQU [""] (
    if exist %~dp0../../etc/kafka/tools-log4j.properties (
        set KAFKA_LOG4J_OPTS=-Dlog4j.configuration=file:%~dp0../../etc/kafka/tools-log4j.properties
    ) else (
        set KAFKA_LOG4J_OPTS=-Dlog4j.configuration=file:%BASE_DIR%/config/tools-log4j.properties
    )
)

rem Classpath additions for Confluent Platform releases
for %%p in (confluent-common kafka-serde-tools monitoring-interceptors) do (
    if exist %BASE_DIR%\share\java\%%p (
        call :concat %BASE_DIR%\share\java\%%p\*
   )
)

"%~dp0kafka-run-class.bat" org.apache.kafka.connect.cli.ConnectDistributed %*
EndLocal

goto :eof
:concat
IF ["%CLASSPATH%"] EQU [""] (
    set "CLASSPATH=%1"
) ELSE (
    set "CLASSPATH=%CLASSPATH%;%1"
)


#####################################
Schemareg.properties

# Copyright 2014 Confluent Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# The address the socket server listens on.
#   FORMAT:
#     listeners = listener_name://host_name:port
#   EXAMPLE:
#     listeners = PLAINTEXT://your.host.name:9092
listeners=http://0.0.0.0:8082

# Zookeeper connection string for the Zookeeper cluster used by your Kafka cluster
# (see zookeeper docs for details).
# This is a comma separated host:port pairs, each corresponding to a zk
# server. e.g. "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002".
kafkastore.connection.url=localhost:2181

# Alternatively, Schema Registry can now operate without Zookeeper, handling all coordination via
# Kafka brokers. Use this setting to specify the bootstrap servers for your Kafka cluster and it
# will be used both for selecting the master schema registry instance and for storing the data for
# registered schemas.
# (Note that you cannot mix the two modes; use this mode only on new deployments or by shutting down
# all instances, switching to the new configuration, and then starting the schema registry
# instances again.)
#kafkastore.bootstrap.servers=PLAINTEXT://localhost:9092

# The name of the topic to store schemas in
kafkastore.topic=_schemas

# If true, API requests that fail will include extra debugging information, including stack traces
debug=false

